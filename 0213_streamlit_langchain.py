#!/usr/bin/env python
# coding: utf-8

# # Streamlitì„ í™œìš©í•´ì„œ RAG ì‹œìŠ¤í…œ êµ¬ì¶•í•˜ê¸°
# langchain ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ êµ¬ì¶•í•´ë³´ì~
# 
# - ì°¸ê³ : https://www.youtube.com/watch?v=xYNYNKJVa4E (ëª¨ë‘ì˜AI ì§±!)

# pip install streamlit

# pip install tiktoken

# pip install langchain

# ### tikoken
# - í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì²­í¬ë¡œ ë‚˜ëˆŒ ë•Œ ë¬¸ìì˜ ê°œìˆ˜ë¥¼ ë¬´ì—‡ì„ ê¸°ì¤€ìœ¼ë¡œ ì‚°ì •ì„ í•  ê²ƒì¸ì§€ì— ëŒ€í•œ ë¶€ë¶„ì„ í† í° ê°œìˆ˜ë¡œ ì…€ ê²ƒì„
# - ë”°ë¼ì„œ **tikoken** ì€ í† í° ê°œìˆ˜ë¥¼ ì„¸ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬

# ### loguru
# - streamlit ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì–´ë–¤ í–‰ë™ì„ ì·¨í–ˆì„ ë•Œ, êµ¬ë™í•œ ê²ƒì´ ë¡œê·¸ë¡œ ë‚¨ë„ë¡ í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬

# ## langchain
# 
# ### ConversationalRetrievalChain
# - ë©”ëª¨ë¦¬ë¥¼ ê°€ì§€ê³  ìˆëŠ” ì²´ì¸
# 
# ### ConversationBufferMemory
# - ëª‡ ê°œê¹Œì§€ì˜ ëŒ€í™”ë¥¼ ë‚´ê°€ ë©”ëª¨ë¦¬ë¡œ ë„£ì–´ì¤„ ê²ƒì¸ì§€ë¥¼ ê²°ì •í•˜ëŠ” ë¶€ë¶„
# 
# ### ChatOpenAI
# - ì˜¤í”ˆAIì˜ APIë¥¼ ê°€ì ¸ì˜¤ê¸° ìœ„í•¨
# 
# ### document_loaders
# - ì—¬ëŸ¬ ìœ í˜•ì˜ ë¬¸ì„œë“¤ì„ ì—¬ëŸ¬ ê°œ ì§‘ì–´ë„£ì–´ë„ ì´ë“¤ì„ ëª¨ë‘ ì´í•´ì‹œí‚¬ ìˆ˜ ìˆëŠ” POCë¥¼ ë§Œë“¤ê²ƒì´ë‹¤~
# - ê·¸ë˜ì„œ ì¢…ë¥˜ë³„ë¡œ ë‹¤ ë¶ˆë €ìŒ
# 
# ### RecursiveCharacterTextSplitter
# - í† í°ì„ ìë¥´ëŠ” splitter
# 
# ### HuggingFaceEmbeddings
# - í•œêµ­ì–´ì— íŠ¹í™”ëœ ì„ë² ë”© ëª¨ë¸
# 
# ### FAISS
# - ì„ì‹œë¡œ ë²¡í„°ë¥¼ ì €ì¥í•˜ëŠ” ë²¡í„° ìŠ¤í† ì–´ 

# In[1]:


import streamlit as st
import tiktoken
from loguru import logger

from langchain.chains import ConversationalRetrievalChain
from langchain.chat_models import ChatOpenAI

from langchain.document_loaders import PyPDFLoader
from langchain.document_loaders import Docx2txtLoader
from langchain.document_loaders import UnstructuredPowerPointLoader

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings

from langchain.memory import ConversationBufferMemory
from langchain.vectorstores import FAISS

# from streamlit_chat import message
from langchain.callbacks import get_openai_callback
from langchain.memory import StreamlitChatMessageHistory


# ## ë©”ì¸ í•¨ìˆ˜
# 1) ë²„íŠ¼ì„ ëˆ„ë¥´ë©´, ì—…ë¡œë“œëœ íŒŒì¼ë“¤ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ê³ , ë²¡í„°í™” ì‹œí‚¤ê³ , ì…ë ¥ëœ openAIì˜ API keyë¥¼ í†µí•´, LLMì„ ë¶ˆëŸ¬ì™€ì„œ, ëª¨ë“  ê²ƒë“¤ì„ í•˜ë‚˜ì˜ ì²´ì¸ìœ¼ë¡œ ì—®ì„ ê²ƒì„!
# 2) ì±„íŒ…í™”ë©´ì„ êµ¬ì„±í•˜ê¸° ìœ„í•œ ì½”ë“œ: ì „ì— í–ˆë˜ ëŒ€í™”ë“¤ë„ ìœ ì§€ê°€ ë˜ê²Œë” í•  ê²ƒì„
# 3) ì§ˆë¬¸ ì°½ì„ êµ¬ì„±í•˜ëŠ” ì½”ë“œ

# In[ ]:


def main():
    st.set_page_config(
    page_title="MJ_Chat", # ì›¹í˜ì´ì§€ íƒ­ ì´ë¦„
    page_icon=":heart-pulse:") # ì›¹í˜ì´ì§€ íƒ­ ì•„ì´ì½˜, :ì“°ë©´ ì•„ì´ì½˜ì´ ë“¤ì–´ê°„ë‹¤

    st.title("_ë¬¼ì–´ë³´ì„¸ìš”! :red[QA Chat Bot]_ ğŸ¤–") # ì›¹í˜ì´ì§€ ì œëª©, _ë¥¼ ì“°ë©´ ê¸°ìš¸ê¸°ì²´ë¡œ ëœë‹¤

    ### 1
    # session_state.conversationë¼ëŠ” ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œ ì´ëŸ° ì‹ìœ¼ë¡œ ì •ì˜ë¥¼ í•´ì¤˜ì•¼í•¨
    if "conversation" not in st.session_state: # session_stateì— conversationì´ ì—†ìœ¼ë©´, 
        st.session_state.conversation = None # session_stateì˜  conversationì„ Noneìœ¼ë¡œ ì„¤ì •

    # session_state.chat_history ë˜í•œ ì±„íŒ… íˆìŠ¤í† ë¦¬ë¥¼ ì €ì¥í•˜ê¸° ìœ„í•¨
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = None

    if "processComplete" not in st.session_state:
        st.session_state.processComplete = None

    # withë¬¸: ì–´ë–¤ êµ¬ì„± ìš”ì†Œ ì•ˆì— í•˜ìœ„ êµ¬ì„± ìš”ì†Œë“¤ì´ ì§‘í–‰ë¼ì•¼í•˜ëŠ” ê²½ìš°ì— í™œìš©ë¨
    # ì‚¬ì´ë“œ ë°”ë¥¼ ë§Œë“œëŠ” ì½”ë“œ
    with st.sidebar:
        uploaded_files =  st.file_uploader("Upload your file",type=['pdf','docx'],accept_multiple_files=True) # file_uploader: íŒŒì¼ ì—…ë¡œë” ê¸°ëŠ¥ì„ ë„£ìŒ
        openai_api_key = st.text_input("OpenAI API Key", key="chatbot_api_key", type="password") # text_input: ì˜¤í”ˆAIì˜ APIë¥¼ ì‘ì„±í•˜ë„ë¡ í•¨
        process = st.button("Process") # button: ë²„íŠ¼ì„ ë§Œë“¦

    # ë§Œì•½ì— processë¼ëŠ” ë²„íŠ¼ì„ ëˆ„ë¥´ë©´~ êµ¬ë™ë˜ëŠ” ë¶€ë¶„
    if process:
        if not openai_api_key: # openAIì˜ APIê°€ ì…ë ¥ë˜ì–´ìˆì§€ ì•Šìœ¼ë©´, ì•„ë˜ ë¬¸êµ¬ë¥¼ ì¶œë ¥í•˜ë„ë¡ í•¨
            st.info("Please add your OpenAI API key to continue.")
            st.stop()
        files_text = get_text(uploaded_files) # ì…ë ¥ë˜ì–´ ìˆë‹¤ë©´, get_textë¥¼ í†µí•´ ì—…ë¡œë“œëœ íŒŒì¼ë“¤ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        text_chunks = get_text_chunks(files_text) # í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ëœ íŒŒì¼ë“¤ì˜ ë¬¸êµ¬ë“¤ì„ ì²­í¬ë¡œ ë‚˜ëˆ„ê¸° ìœ„í•¨
        vetorestore = get_vectorstore(text_chunks) # ë²¡í„°í™” ì‹œí‚´
     
        st.session_state.conversation = get_conversation_chain(vetorestore,openai_api_key)  # ë²¡í„° ìŠ¤í† ì–´ë¥¼ ê°–ê³  LLMì´ ë‹µë³€ì„ í•  ìˆ˜ ìˆë„ë¡ ì²´ì¸ì„ êµ¬ì„±

        st.session_state.processComplete = True

    ### 2 
    if 'messages' not in st.session_state: # ë¨¼ì € ì•„ë˜ì™€ ê°™ì€ ë¬¸êµ¬ë¥¼ ì¶œë ¥í•¨ìœ¼ë¡œì¨, UIì ìœ¼ë¡œ ì¹œìˆ™í•œ í™”ë©´ì„ ë§Œë“¤ ìˆ˜ ìˆìŒ
        st.session_state['messages'] = [{"role": "assistant", 
                                        "content": "ì•ˆë…•í•˜ì„¸ìš”! ì‚¬ì´ë“œ ë°”ì— ë¬¸ì„œë¥¼ ë„£ì€ í›„, í•´ë‹¹ ë¬¸ì„œì— ëŒ€í•´ ê¶ê¸ˆí•œ ê²ƒì„ ë¬¼ì–´ë³´ì„¸ìš”ğŸ˜ƒ ì–´ë–¤ íŒŒì¼ ìœ í˜•ì´ë“  ë‹¤ ì½ì„ ìˆ˜ ìˆì–´ìš”! ë‹¨, ë¨¼ì € OPENAIì˜ APIë¥¼ ë„£ì–´ì•¼í•œë‹¤ëŠ” ì  ìŠì§€ë§ˆì„¸ìš”!"}]

    # ì–´ë–¤ ì—­í• ì„ ë§¡ì€ ì•„ì´ì½˜ì„ í•¨ê»˜ í‘œì‹œë¥¼ í•´ì£¼ê³ , ì»¨í…Œì´ë„ˆ ì•ˆì— contentì— í•´ë‹¹í•˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ ì ê¸° ìœ„í•¨ 
    # í•œ ë²ˆ ë©”ì„¸ì§€ê°€ ì…ë ¥ë ë•Œë§ˆë‹¤, í•˜ë‚˜ì˜ contentë¡œ ì—®ìŒ
    # forë¬¸ìœ¼ë¡œ êµ¬ì„±í–ˆê¸° ë•Œë¬¸ì—, ë©”ì„¸ì§€ê°€ í•˜ë‚˜ ì˜¬ë¼ì˜¬ë•Œ ë§ˆë‹¤ ì•„ë˜ ì½”ë“œë¥¼ ìˆ˜í–‰í•¨
    for message in st.session_state.messages: # session_state.messagesì—ì„œ messageë“¤ë§ˆë‹¤ 
        with st.chat_message(message["role"]): # withë¬¸ìœ¼ë¡œ ë¬¶ì–´ì¤„ê±°ë‹¤
            st.markdown(message["content"]) # messageì˜ roleì— ë”°ë¼ì„œ ë©”ì„¸ì§€ì˜ contentë¥¼ ë§ˆí¬ë‹¤ìš´ í• ê±°ë‹¤!

    # LLMì´ ë©”ëª¨ë¦¬ë¥¼ ê°–ê³ , contextë¥¼ ê³ ë ¤í•´ì„œ ë‹µë³€í•˜ê¸° ìœ„í•¨
    history = StreamlitChatMessageHistory(key="chat_messages")

    ### 3
    # Chat logic
    # ì‚¬ìš©ìê°€ ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´, ifë¬¸ì„ ì‹œì‘í•¨
    if query := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."):
        st.session_state.messages.append({"role": "user", "content": query}) # ì§ˆë¬¸ì„ í•˜ê²Œ ëœë‹¤ë©´, ê°–ê³ ìˆëŠ” session_state.messagesì— userì˜ roleì„ ì²˜ìŒ ì¶”ê°€

        # contentë¡œëŠ” queryë¥¼ ë‚ ë ¤ì£¼ê²Œ ë¨ > ì²« ë²ˆì§¸ ì§ˆë¬¸ì´ queryê°€ ëœë‹¤ëŠ” ëœ»
        # ì§ˆë¬¸ì´ ì…ë ¥ë˜ë©´, session_state.messages
        with st.chat_message("user"): # userë¼ëŠ” í‘œì‹œëŠ” ì•„ì´ì½˜ì„ í†µí•´ì„œ í™•ì¸í•  ìˆ˜ ìˆìŒ
            st.markdown(query) # ë³´ë‚¸ ì§ˆë¬¸ì„ ë§ˆí¬ë‹¤ìš´ 

        # ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë°›ì•„ì„œ ì–´ì‹œìŠ¤í„´íŠ¸ê°€ ëŒ€ë‹µ
        with st.chat_message("assistant"):
            chain = st.session_state.conversation

            # ë¡œë”©í• ë•Œ ë™ê·¸ë¼ë¯¸ê°€ ëŒì•„ê°€ëŠ” ë¶€ë¶„
            with st.spinner("Thinking..."):
                result = chain({"question": query})
                with get_openai_callback() as cb:
                    st.session_state.chat_history = result['chat_history']  # ì±„íŒ… ê¸°ë¡ ì €ì¥
                response = result['answer']
                source_documents = result['source_documents'] # ì°¸ê³ í•œ ë¬¸ì„œ ì €ì¥

                st.markdown(response)
                with st.expander("ì°¸ê³  ë¬¸ì„œ í™•ì¸"): # ì°¸ê³  ë¬¸ì„œë¥¼ ì ‘ì—ˆë‹¤ íˆë‹¤ í‘œì‹œí•˜ëŠ” ë¶€ë¶„
                    st.markdown(source_documents[0].metadata['source'], help = source_documents[0].page_content)
                    st.markdown(source_documents[1].metadata['source'], help = source_documents[1].page_content)
                    st.markdown(source_documents[2].metadata['source'], help = source_documents[2].page_content)
                    


        # Add assistant message to chat history
        # ì–´ì‹œìŠ¤í„´íŠ¸ê°€ ë‹µë³€í•œ ê²ƒë„ ê¸°ë¡
        st.session_state.messages.append({"role": "assistant", "content": response})


# ---
# ## ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜

# In[ ]:


# í† í° ê°œìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ splitting í•´ì£¼ëŠ” í•¨ìˆ˜ 
def tiktoken_len(text):
    tokenizer = tiktoken.get_encoding("cl100k_base")
    tokens = tokenizer.encode(text)
    return len(tokens)


# - ë¡œì»¬ íŒŒì¼ì¸ ê²½ìš°ì—ëŠ” íŒŒì¼ì´ ì €ì¥ëœ ê²½ë¡œë¥¼ ë„£ì–´ì£¼ë©´ ë¡œë”©ì´ ë¨
# - ê·¸ëŸ¬ë‚˜ streamlitì˜ ê²½ìš°, í´ë¼ìš°ë“œ ìƒì—ì„œ ì‘ë™ì´ ë˜ëŠ”ë°, ì‚¬ëŒë“¤ì´ ìì‹ ì˜ íŒŒì¼ì„ ë„£ì—ˆì„ ë•Œ, ê·¸ íŒŒì¼ë“¤ì— ëŒ€í•´ì„œ ì±„íŒ…ì´ ê°€ëŠ¥í•˜ë„ë¡ ë§Œë“œëŠ” ê²ƒ
# - streamlit í´ë¼ìš°ë“œ ìƒì—ì„œ ëŒì•„ê°€ëŠ” ì„ì‹œ ì €ì¥ ê²½ë¡œë¥¼ ì œê³µ > ë¹ˆ íŒŒì¼ì´ ìƒê¸°ê²Œ ë¨ > ë¹ˆ íŒŒì¼ì— ì§„ì§œë¡œ ë¶ˆëŸ¬ì˜¨ ì—…ë¡œë“œí•œ íŒŒì¼ì˜ ê°’ì„ ë„£ì–´ì¤Œ
# - ìš°íšŒì ìœ¼ë¡œ ì—…ë¡œë“œí•œ íŒŒì¼ì˜ ê°’ì„ ë„£ì„ ìˆ˜ ìˆê²Œ ë¨

# In[ ]:


# ì—…ë¡œë“œëœ íŒŒì¼ë“¤ì„ ëª¨ë‘ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ 
def get_text(docs):

    doc_list = []
    
    for doc in docs:
        file_name = doc.name  # doc ê°ì²´ì˜ ì´ë¦„ì„ íŒŒì¼ ì´ë¦„ìœ¼ë¡œ ì‚¬ìš©
        with open(file_name, "wb") as file:  # íŒŒì¼ì„ doc.nameìœ¼ë¡œ ì €ì¥
            file.write(doc.getvalue())
            logger.info(f"Uploaded {file_name}")
        if '.pdf' in doc.name:
            loader = PyPDFLoader(file_name)
            documents = loader.load_and_split()
        elif '.docx' in doc.name:
            loader = Docx2txtLoader(file_name)
            documents = loader.load_and_split()
        elif '.pptx' in doc.name:
            loader = UnstructuredPowerPointLoader(file_name)
            documents = loader.load_and_split()

        doc_list.extend(documents) # doc_listì— ë‹´ì•„ì„œ í•˜ë‚˜ì˜ ë‹¤íë¨¼íŠ¸ ëª©ë¡ì„ ë§Œë“¤ì–´ì„œ ë¦¬í„´í•´ì¤Œ
    return doc_list


# In[ ]:


# ì—¬ëŸ¬ ì²­í¬ë“¤ë¡œ splití•˜ëŠ” ê³¼ì •
def get_text_chunks(text):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=900, # í† í° ê°œìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ 900 ì‚¬ì´ì¦ˆë¥¼ ê°–ëŠ” ì²­í¬ë¡œ ë‚˜ëˆ ì¤Œ
        chunk_overlap=100, # ì• ë’¤ë¡œ ê²¹ì¹˜ëŠ” ë¶€ë¶„ì„ 100ì”© ë§Œë“¤ì–´ì¤Œ
        length_function=tiktoken_len
    )
    chunks = text_splitter.split_documents(text) # ì—¬ëŸ¬ê°œì˜ ì²­í¬ë“¤ë¡œ splitting í•¨
    return chunks


# #### ì„ë² ë”© í•˜ëŠ” ê³¼ì •
# - í…ìŠ¤íŠ¸ë“¤ì„ ìˆ˜ì¹˜í™”í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ê²ƒì´ ì„ë² ë”© ëª¨ë¸ì„
# - ko-sroberta-multitask: í—ˆê¹… í˜ì´ìŠ¤ì— ìˆëŠ” ì„ë² ë”© ëª¨ë¸
# - FAISS: ë²¡í„° ì €ì¥ì†Œ
# - ì–´ë–¤ í…ìŠ¤íŠ¸ë“¤ì´ FAISS ë²¡í„° ì €ì¥ì†Œë¥¼ ì •ë¦¬í• ì§€ ë§í•´ì£¼ê¸° ìœ„í•´ì„œ ì…ë ¥ ë°›ì€ ë§¤ê°œ ë³€ìˆ˜ì¸ text_chunksë¥¼ ë„£ê³ , ìœ„ì—ì„œ ì •ì˜í•œ ì„ë² ë”© ëª¨ë¸ì„ ë„£ì–´ì£¼ë©´, í…ìŠ¤íŠ¸ ì²­í¬ì— ëŒ€í•´ì„œ ì„ë² ë”© ëª¨ë¸ë¡œ ìˆ˜ì¹˜í™”í•˜ëŠ” ê³¼ì •ì´ ì•„ë˜ í•¨ìˆ˜ë¡œ ì‹¤í–‰ë¨

# In[ ]:


# ë²¡í„° ìŠ¤í† ì–´ í•¨ìˆ˜
def get_vectorstore(text_chunks):
    embeddings = HuggingFaceEmbeddings(
                                        model_name="jhgan/ko-sroberta-multitask",
                                        model_kwargs={'device': 'cpu'}, # streamlit í´ë¼ìš°ë“œ ìƒì—ì„œ ëŒì•„ê°€ëŠ” pcëŠ” GPUê°€ ì—†ì–´ì„œ CPUë¡œ
                                        encode_kwargs={'normalize_embeddings': True} # ë²¡í„° ì €ì¥ì†Œì— ì €ì¥ì„ í•´ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ë¹„êµí•˜ê¸° ìœ„í•´ 
                                        )  
    vectordb = FAISS.from_documents(text_chunks, embeddings) 
    return vectordb 


# ìœ„ì—ì„œ ì„ ì–¸í•œ ëª¨ë“  ê²ƒë“¤ì„ ë‹¤ ë‹´ëŠ” í•¨ìˆ˜
# - ConversationBufferMemory: chat_historyë¼ëŠ” í‚¤ ê°’ì„ ë©”ëª¨ë¦¬ë¡œ ì €ì¥í•  ê²ƒì„

# In[ ]:


def get_conversation_chain(vetorestore,openai_api_key):
    llm = ChatOpenAI(openai_api_key=openai_api_key, model_name = 'gpt-3.5-turbo',temperature=0)
    conversation_chain = ConversationalRetrievalChain.from_llm(
            llm=llm, 
            chain_type="stuff", 
            retriever=vetorestore.as_retriever(search_type = 'mmr', vervose = True), 
            memory=ConversationBufferMemory(memory_key='chat_history', return_messages=True, output_key='answer'), # ë‹µë³€í•˜ëŠ” ë¶€ë¶„ë§Œ íˆìŠ¤í† ë¦¬ì— ë‹´ê² ë‹¤
            get_chat_history=lambda h: h,
            return_source_documents=True,
            verbose = True
        )

    return conversation_chain


# In[ ]:


if __name__ == '__main__':
    main()


# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:




